{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Chapter3_Grid World\n",
    "모두의 연구소 flipped school - advanced reinforcement learning 의 교재인 \n",
    "Sutton Reinforcement Learning 책에 나오는 Chapter3. 밴딧 문제\n",
    "\n",
    "코드는 \n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  \n",
    "2016 Kenta Shimada(hyperkentakun@gmail.com)  \n",
    "\n",
    "---\n",
    "의 코드를 참고하였습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "넘파이의 함수를 통해 표준분포 랜덤값을 생성하고 인수로 받는 수보다 크면 1을 작으면 -1을 받는 함수를 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 * 5 짜리 그리드월드를 만들어 줍니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "height,weight=5,5\n",
    "A_POS=[0,1]\n",
    "\n",
    "A_PRIME_POS=[4,1]\n",
    "B_POS=[0,3]\n",
    "B_PIRME_POS=[2,3]\n",
    "discount=0.9\n",
    "world=np.zeros((height,weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  action 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리드월드에서 할수있는 action 으로 위,아래,오른쪽,왼쪽을 설정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions=[\"L\",\"R\",\"D\",\"U\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default 값으로 각 그리드에서 각 엑션을 선택할 확률을 25%로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionProb=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    actionProb.append([])\n",
    "    for j in range(0, 5):\n",
    "        actionProb[i].append(dict({'L':0.25, 'U':0.25, 'R':0.25, 'D':0.25}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25}],\n",
       " [{'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25}],\n",
       " [{'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25}],\n",
       " [{'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25}],\n",
       " [{'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25},\n",
       "  {'D': 0.25, 'L': 0.25, 'R': 0.25, 'U': 0.25}]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actionProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Next State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 밴딧 문제와 다르게 그리드월드에서는 각 스테이트에서 어떠한 엑션을 선택했냐에 따라 다음의 스테이트가 변합니다.\n",
    "각 스테이트에서 엑션을 선택했을때 받는 다음 스테이트를 정의해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextState=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마찬가지로 각 스테이트에서의 엑션 선택에 따라 받는 리워드도 달라집니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionReward=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    nextState.append([])\n",
    "    actionReward.append([])\n",
    "   \n",
    "    for j in range(0,5):\n",
    "        next=dict()\n",
    "        reward=dict()\n",
    "        # 만약 0,0에 있을때 엑션이 윗방향이라면 그 자리에서 움직이지 않고 리워드는 -1을 받는다\n",
    "        #참고 i는 row ,j는 column\n",
    "        if i ==0:\n",
    "           \n",
    "            next['U'] = [i,j]\n",
    "            \n",
    "            reward['U'] = -1.0\n",
    "            \n",
    "        else:\n",
    "            next['U']=[i-1,j]\n",
    "            reward['U']=0.0\n",
    "        if i==4:\n",
    "            next['D']=[i,j]\n",
    "            reward['D']=-1.0\n",
    "        \n",
    "        else:\n",
    "            next['D']=[i+1,j]\n",
    "            reward['D']=0.0\n",
    "        if j==0:\n",
    "            next['L']=[i,j]\n",
    "            reward['L']=-0.1\n",
    "        else:\n",
    "            next['L']=[i,j-1]\n",
    "            reward['L']=0.0\n",
    "        if j==4:\n",
    "            next['R']=[i,j]\n",
    "            reward['R']=-0.1\n",
    "        else:\n",
    "            next['R']=[i,j+1]\n",
    "            reward['R']=0.0\n",
    "        if [i,j]==A_POS:\n",
    "            next['L'] = next['R'] = next['D'] = next['U'] = A_PRIME_POS \n",
    "            reward['L'] = reward['R'] = reward['D'] = reward['U'] = 5.0\n",
    "            \n",
    "        nextState[i].append(next)\n",
    "        actionReward[i].append(reward)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D': [4, 4], 'L': [4, 3], 'R': [4, 4], 'U': [3, 4]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D': -1.0, 'L': 0.0, 'R': -0.1, 'U': 0.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of using dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': [5, 2]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=dict()\n",
    "\n",
    "a['a']=[5,2]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "벨만방정식으로 업데이트를 하면서 월드와 업데이트한 뉴월드의 오차가 매우 작으면 멈춘다! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.46159439  4.01167018  0.86462721 -0.35305208 -0.82570799]\n",
      " [ 0.78332305  1.2217614   0.43063814 -0.14389164 -0.44313827]\n",
      " [ 0.1258528   0.20440403 -0.02856495 -0.27398145 -0.44566969]\n",
      " [-0.44313827 -0.41060213 -0.48803134 -0.59958433 -0.70686891]\n",
      " [-1.13051998 -1.09814806 -1.13029202 -1.19595281 -1.27842082]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "while True:\n",
    "    #until convergence\n",
    "    newWorld=np.zeros((5,5))\n",
    "    somany+=1\n",
    "    for i in range(0,5):\n",
    "        for j in range(0,5):\n",
    "            for action in actions:\n",
    "                newPosition=nextState[i][j][action]\n",
    "                #bellman\n",
    "         \n",
    "                newWorld[i, j] += actionProb[i][j][action] * (actionReward[i][j][action] + discount * world[newPosition[0], newPosition[1]])\n",
    "                \n",
    "    if np.sum(np.abs(world-newWorld))<1e-4:\n",
    "        \n",
    "    \n",
    "\n",
    "        print(newWorld)\n",
    "        break\n",
    "    world=newWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.98870716  12.20967462  10.98870716   9.88983644   8.9008528 ]\n",
      " [  9.88983644  10.98870716   9.88983644   8.9008528    8.01074958]\n",
      " [  8.9008528    9.88983644   8.9008528    8.01074958   7.20967462]\n",
      " [  8.01074958   8.9008528    8.01074958   7.20967462   6.48870716]\n",
      " [  7.20967462   8.01074958   7.20967462   6.48870716   5.83983644]]\n"
     ]
    }
   ],
   "source": [
    "world=np.zeros((5,5))\n",
    "while True:\n",
    "    newWorld=np.zeros((5,5))\n",
    "    for i in range(0,5):\n",
    "        for j in range(0,5):\n",
    "            values=[]\n",
    "            for action in actions:\n",
    "                newPosition=nextState[i][j][action]\n",
    "                #value iteration\n",
    "                values.append(actionReward[i][j][action] + discount * world[newPosition[0], newPosition[1]])\n",
    "            newWorld[i][j] =np.max(values)\n",
    "    if np.sum(np.abs(world-newWorld))<1e-4:\n",
    "  \n",
    "        print(newWorld)\n",
    "        break\n",
    "    world=newWorld\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
